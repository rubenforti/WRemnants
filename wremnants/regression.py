import numpy as np
from scipy import stats
from scipy.optimize import nnls
from scipy.special import comb

from utilities import logging

logger = logging.child_logger(__name__)


def compute_chi2(y, y_pred, w=None, nparams=1):
    # goodness of fit from parameter matrix 'X', values 'y' and parateters 'params'
    residuals = (y - y_pred)
    if w is not None:
        residuals *= w
    chi2 = np.sum((residuals**2), axis=-1) # per fit chi2
    chi2_total = np.sum((residuals**2)) # chi2 of all fits together

    # Degrees of freedom calculation
    ndf = y.shape[-1] - nparams
    ndf_total = y.size - chi2.size*nparams

    logger.info(f"Total chi2/ndf = {chi2_total}/{ndf_total} = {chi2_total/ndf_total} (p = {stats.chi2.sf(chi2_total, ndf_total)})")
    logger.info(f"Min chi2 = {chi2.min()} (p = {stats.chi2.sf(chi2.min(), ndf)})")
    logger.info(f"Max chi2 = {chi2.max()} (p = {stats.chi2.sf(chi2.max(), ndf)})")
    logger.info(f"Mean chi2 = {chi2.mean()}")
    logger.info(f"Std chi2 = {chi2.std()}")
    return chi2, ndf    


def get_parameter_eigenvectors(params, cov, sign=1, force_positive=False):
    # diagonalize and get eigenvalues and eigenvectors
    e, v = np.linalg.eigh(cov) # The column eigenvectors[:, i] is the normalized eigenvector corresponding to the eigenvalue eigenvalues[i], 
    vT = np.transpose(v, axes=(*np.arange(v.ndim-2), v.ndim-1, v.ndim-2)) # transpose v to have row eigenvectors[i, :] for easier computations
    mag = np.sqrt(e)[...,np.newaxis] * vT * sign
    # duplicate params vector to have duplicated rows
    params_brd = np.broadcast_to(params[...,np.newaxis], mag.shape)
    paramsT = np.transpose(params_brd, axes=(*np.arange(params_brd.ndim-2), params_brd.ndim-1, params_brd.ndim-2))
    mask = (paramsT + mag < 0).any(axis=-1)
    if force_positive and mask.sum() > 0:
        logger.info(f"Force {mask.sum()} eigenvectors to be positive")
        # scaling the magnitude of the eigenvector to avoid negative coefficients
        min_idxs = paramsT + mag == np.min(paramsT + mag, axis=-1)[...,np.newaxis] # find indices with minimum value of each eigenvector
        min_paramsT = paramsT[min_idxs]
        min_mag = mag[min_idxs]
        factors = np.reshape(-1*min_paramsT/min_mag, mask.shape)
        factors = np.where(mask, factors, np.ones_like(mask))
        factors = np.broadcast_to(factors[...,np.newaxis], mag.shape)
        mag = factors * mag
    if np.sum(mag.sum(axis=-1) == 0):
        logger.warning(f"Found {np.sum(mag.sum(axis=-1) == 0)} eigenvector shifts where all coefficients are 0") 
    params_var = paramsT + mag
    if np.sum(params_var.sum(axis=-1) == 0):
        logger.warning(f"Found {np.sum(params_var.sum(axis=-1) == 0)} variations where all coefficients are 0") 
    return params_var


def make_eigenvector_predictons(params, cov, func, x1, x2=None, force_positive=False):
    # return alternate values i.e. nominal+/-variation
    params_up = get_parameter_eigenvectors(params, cov, sign=1, force_positive=force_positive)
    y_pred_up = func(x1, params_up) if x2 is None else func(x1, x2, params_up)
    y_pred_up = np.moveaxis(y_pred_up, params.ndim-1, -1) # put parameter variations last
    params_dn = get_parameter_eigenvectors(params, cov, sign=-1, force_positive=force_positive)
    y_pred_dn = func(x1, params_dn) if x2 is None else func(x1, x2, params_dn)
    y_pred_dn = np.moveaxis(y_pred_dn, params.ndim-1, -1) # put parameter variations last
    return np.stack((y_pred_up, y_pred_dn), axis=-1)


def poly(pol, order, order2=None):
    if order2 is None:
        if pol=="power":
            return lambda x, n, p=1: p * x**n
        elif pol=="bernstein":
            return lambda x, n, p=1, o=order: p * comb(o, n) * x**n * (1 - x)**(o - n)
        elif pol=="monotonic":
            # integral of bernstein (see https://en.wikipedia.org/wiki/Bernstein_polynomial#Properties)
            # for n>o return one additional parameter for the constant term from the integration
            return lambda x, n, p=1, o=order: p*x**0 if n==0 else -1*p * 1/o * np.array(
                    [comb(o, k) * x**k * (1 - x)**(o - k) for k in range(n, o+1)]
                ).sum(axis=0)
    else:
        return lambda x, x2, n, m, p=1, o1=order, o2=order2: p * poly(pol, o1)(x, n) * poly(pol, o2)(x2, m)


def get_parameter_matrices(x, y, w, order, pol="power"):
    if x.shape != y.shape:
        x = np.broadcast_to(x, y.shape)
    stackX=[] # parameter matrix X 
    stackXTY=[] # and X.T @ Y
    f = poly(pol, order)
    for n in range(order+1):
        p = f(x, n)
        stackX.append( w * p )
        stackXTY.append((w**2 * p * y).sum(axis=(-1)))
    X = np.stack(stackX, axis=-1)
    XTY = np.stack(stackXTY, axis=-1)
    return X, XTY


def get_parameter_matrices_from2D(x, x2, y, w, order, order2=None, pol="power", flatten=False):
    x, x2 = np.broadcast_arrays(x[np.newaxis,...], x2[..., np.newaxis])
    x = np.broadcast_to(x, y.shape)
    x2 = np.broadcast_to(x2, y.shape)
    if order2 is None:
        order2 = [0,]*(order+1)
    elif type(order2) == int:
        order2 = [order2,]*(order+1)
    elif type(order2) == list: 
        if len(order2) < order+1:
            order2.append([0,]*(len(order2)-order+1))
    else:
        raise RuntimeError(f"Input 'order2' requires type 'None', 'int' or 'list'")
    stackX=[]   # parameter matrix X 
    stackXTY=[] # and X.T @ Y
    for n in range(order+1):
        f = poly(pol, order, order2[n])
        for m in range(order2[n]+1):
            p = f(x, x2, n, m)
            stackX.append(w * p)
            stackXTY.append((w**2 * p * y).sum(axis=(-2, -1)))
    X = np.stack(stackX, axis=-1)
    XTY = np.stack(stackXTY, axis=-1)
    if flatten:
        # flatten the 2D array into 1D
        newshape = (*y.shape[:-2],np.product(y.shape[-2:]))
        y = y.reshape(newshape)
        X = X.reshape(*newshape, X.shape[-1])

    return X, y, XTY


def get_regression_function(orders, pol="power"):
    if not isinstance(orders, list):
        orders = [orders]

    if len(orders) == 1:
        def fsum(x, ps, o=orders[0]):
            f = poly(pol, o)
            if hasattr(ps, "ndim") and ps.ndim > 1:
                return sum([f(x, n, ps[...,n,np.newaxis]) for n in range(o+1)])
            else:
                return sum([f(x, n, ps[n]) for n in range(o+1)])
    else:
        def fsum(x1, x2, ps, o1=orders[0], o2=orders[1]):
            idx=0
            psum = 0
            x1, x2 = np.broadcast_arrays(x1[np.newaxis,...], x2[..., np.newaxis])
            if hasattr(ps, "ndim") and ps.ndim > 1:
                x1 = np.broadcast_to(x1, [*ps.shape[:-1], *x1.shape])
                x2 = np.broadcast_to(x2, [*ps.shape[:-1], *x2.shape]) 
                for n in range(o1+1):
                    f = poly(pol, o1, o2[n])
                    for m in range(o2[n]+1):
                        psum += f(x1, x2, n, m, ps[...,idx,np.newaxis,np.newaxis])
                        idx += 1
            else:
                for n in range(o1+1):
                    f = poly(pol, o1, o2[n])
                    for m in range(o2[n]+1):
                        psum += f(x1, x2, n, m, ps[idx])
                        idx += 1
            return psum
    return fsum


def solve_leastsquare(X, XTY):
    # compute the transpose of X for the mt and parameter axes 
    XT = np.transpose(X, axes=(*np.arange(X.ndim-2), X.ndim-1, X.ndim-2))
    XTX = XT @ X
    # compute the inverse of the matrix in each bin (reshape to make last two axes contiguous, reshape back after inversion), 
    # this term is also the covariance matrix for the parameters
    XTXinv = np.linalg.inv(XTX.reshape(-1,*XTX.shape[-2:]))
    XTXinv = XTXinv.reshape((*XT.shape[:-2],*XTXinv.shape[-2:])) 
    params = np.einsum('...ij,...j->...i', XTXinv, XTY)
    return params, XTXinv


def solve_nonnegative_leastsquare(X, XTY, exclude_idx=None):
    # exclude_idx to exclude the non negative constrained for one parameter by evaluating the nnls twice and flipping the sign
    XT = np.transpose(X, axes=(*np.arange(X.ndim-2), X.ndim-1, X.ndim-2))
    XTX = XT @ X
    XTXinv = np.linalg.inv(XTX.reshape(-1,*XTX.shape[-2:]))
    XTXinv = XTXinv.reshape((*XT.shape[:-2],*XTXinv.shape[-2:])) 
    orig_shape = XTY.shape
    nBins = np.prod(orig_shape[:-1])
    XTY_flat = XTY.reshape(nBins, XTY.shape[-1])
    XTX_flat = XTX.reshape(nBins, XTX.shape[-2], XTX.shape[-1])
    # params = [fnnls(xtx, xty) for xtx, xty in zip(XTX_flat, XTY_flat)] # use fast nnls (for some reason slower, even though it should be faster ...)
    params = [nnls(xtx, xty)[0] for xtx, xty in zip(XTX_flat, XTY_flat)] # use scipy implementation of nnls
    params = np.reshape(params, orig_shape)
    if exclude_idx is not None and np.sum(params[...,exclude_idx]==0):
        mask = params[...,exclude_idx]==0
        mask_flat = mask.flatten()
        w_flip = np.ones(XTY.shape[-1])
        w_flip[exclude_idx] = -1
        params_negative = [nnls(xtx, xty)[0] for xtx, xty in zip(XTX_flat[mask_flat], XTY_flat[mask_flat] * w_flip)]
        params[mask] = np.array(params_negative) * w_flip
        logger.info(f"Found {mask.sum()} parameters that are excluded in nnls and negative")
    return params, XTXinv


def get_solver(polynomial):
    # solve with non negative least squares
    if polynomial in ["bernstein",]:
        return solve_nonnegative_leastsquare
    elif polynomial in ["monotonic",]:
        # exclude first parameter (integration constant) from non negative constraint
        return lambda x,y: solve_nonnegative_leastsquare(x,y,0)
    else:
        return solve_leastsquare


def transform_bernstein(x, min_x, max_x, cap_x=False):
    # get x axes values for interpolation/smoothing with transformation
    # transform bernstein polinomials to [0,1]
    x = (x - min_x) / (max_x - min_x)
    if np.sum(x < 0) or np.sum(x > 1):
        if cap_x:
            logger.info("Values outside [0,1] found, those will be capped to [0,1]")
            x[x < 0] = 0
            x[x > 1] = 1
        else:
            raise RuntimeError(f"All values need to be within [0,1] but {np.sum(x < 0)} values smaller 0 ({x[x < 0]}) and {np.sum(x > 1)} larger 1 ({x[x > 1]}) found after transformation with xmin={xmin} and xmax={xmax}")
    return x


class Regressor(object):
    def __init__(
        self, 
        polynomial, 
        order=None, 
        order2=None,
        cap_x=False,
        min_x=0,
        max_x=1,
    ):
        self.polynomial = polynomial
        self.order = order

        self.solver = get_solver(polynomial)

        self.evaluator = get_regression_function(order, pol=polynomial)

        self.cap_x = cap_x
        self.min_x = min_x
        self.max_x = max_x

        self.params = None
        self.cov = None

        # external parameters and covariance matrix added for evaluation
        self.external_params = None
        self.external_cov = None

    def transform_x(self, x):
        # if self.polynomial in ["bernstein", "monotonic"]:
        x = transform_bernstein(x, self.min_x, self.max_x, self.cap_x)
        return x

    def solve(self, x, y, w, chi2_info=True):
        x = self.transform_x(x)
        X, XTY = get_parameter_matrices(x, y, w, self.order, pol=self.polynomial)
        self.params, self.cov = self.solver(X, XTY)

        if chi2_info:
            ypred = self.evaluator(x, self.params)
            compute_chi2(y, ypred, w, self.params.shape[-1])

    def evaluate(self, x):
        x = self.transform_x(x)
        params = self.params
        if self.external_params is not None:
            params += self.external_params[..., *[np.newaxis for n in range(self.params.ndim - self.external_params.ndim)],:]            
        return self.evaluator(x, params)

    def get_eigenvector_predictions(self, x1, x2=None):
        x1 = self.transform_x(x1)
        if x2 is not None:
            x2 = self.transform_x(x2)
        cov = self.cov
        params = self.params
        if self.external_cov:
            cov += self.external_cov[..., *[np.newaxis for n in range(self.cov.ndim - self.external_cov.ndim)],:,:]
        if self.external_params is not None:
            params += self.external_params[..., *[np.newaxis for n in range(self.params.ndim - self.external_params.ndim)],:]
        return make_eigenvector_predictons(params, cov, func=self.evaluator, x1=x1, x2=x2, force_positive=False)

    def force_positive(self, exclude_idx=None):
        # performing a nnls to enforce monotonicity for the signal region (using generalized least squares)
        Y = self.params
        W = np.linalg.inv(self.cov.reshape(-1,*self.cov.shape[-2:]))
        W = W.reshape((*self.cov.shape[:-2],*W.shape[-2:])) 
        WY = np.einsum('...ij,...j->...i', W, Y)
        # the design matrix X is just a 1xn unity matrix and can thus be ignored
        XTWY = WY
        XTWX = W

        orig_shape = XTWY.shape
        nBins = np.prod(orig_shape[:-1])
        XTWY_flat = XTWY.reshape(nBins, XTWY.shape[-1])
        XTWX_flat = XTWX.reshape(nBins, XTWX.shape[-2], XTWX.shape[-1])
        self.params = [nnls(xtwx, xtwy)[0] for xtwx, xtwy in zip(XTWX_flat, XTWY_flat)]
        self.params = np.reshape(self.params, orig_shape)

        # allow the integration constaint to be negative
        if exclude_idx is not None and np.sum(self.params[...,exclude_idx]==0) > 0:
            mask = self.params[...,exclude_idx]==0
            mask_flat = mask.flatten()
            w_flip = np.ones(XTWY.shape[-1])
            w_flip[exclude_idx] = -1
            self.params_negative = [nnls(xtx, xty)[0] for xtx, xty in zip(XTWX_flat[mask_flat], XTWY_flat[mask_flat] * w_flip)]
            self.params[mask] = np.array(self.params_negative) * w_flip
            logger.info(f"Found {mask.sum()} parameters that are excluded in nnls and negative")


class Regressor2D(Regressor):
    def __init__(
        self, 
        *args,
        **kwargs
    ):
        super().__init__(*args, **kwargs)

        if not isinstance(self.cap_x, list):
            self.order = [self.order] * 2
        if not isinstance(self.cap_x, list):
            self.cap_x = [self.cap_x] * 2
        if not isinstance(self.min_x, list):
            self.min_x = [self.min_x] * 2
        if not isinstance(self.max_x, list):
            self.max_x = [self.max_x] * 2

    def transform_x(self, x1, x2):
        if self.polynomial in ["bernstein", "monotonic"]:
            x1 = transform_bernstein(x1, self.min_x[0], self.max_x[0], self.cap_x[0])
            x2 = transform_bernstein(x2, self.min_x[1], self.max_x[1], self.cap_x[1])
        return x1, x2

    def solve(self, x1, x2, y, w, flatten=False):
        x1, x2 = self.transform_x(x1, x2)
        X, XTY = get_parameter_matrices_from2D(x1, x2, y, w, *self.order, pol=self.polynomial, flatten=flatten)
        self.params, self.cov = self.solver(X, XTY)
